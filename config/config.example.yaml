# RAG System Configuration

database:
  url: "postgresql://rag_user:rag_password@postgres:5432/rag_db"
  pool_size: 10
  max_overflow: 20

models:
  embedding:
    model_name: "sentence-transformers/all-MiniLM-L12-v2"
    device: "cpu"  # or "cuda"
    batch_size: 32
  
  reranker:
    model_name: "cross-encoder/ms-marco-MiniLM-L-12-v2"
    device: "cpu"
    batch_size: 16
  
  llm:
    provider: "ollama"  # ollama, openai, azure
    model: "llama3.2:3b"
    base_url: "http://ollama:11434"
    temperature: 0.1
    max_tokens: 1000
  
  vision:
    provider: "ollama"
    model: "llava:7b"
    base_url: "http://ollama:11434"

search:
  hybrid:
    vector_weight: 0.7
    keyword_weight: 0.3
    top_k_candidates: 25
  
  rerank:
    top_k: 10
  
  evidence:
    min_score: 0.75
    insufficient_threshold: 2
    medium_threshold: 2
    high_threshold: 3
    max_sources: 5

ingestion:
  git:
    url: "https://gitlab.platform.ks.gbpiweb.loc/docs/teamhelper/team-helper.git"
    branch: "main"
    local_path: "/data/docs"
  
  markdown:
    extract_code_blocks: true
    extract_images: true
  
  chunking:
    strategy: "section"  # section, fixed, semantic
    max_tokens: 512

api:
  cors_origins:
    - "http://localhost:3000"
    - "https://backstage.bpi.pt"
  
  rate_limit_enabled: false
  rate_limit_requests_per_minute: 60

logging:
  level: "INFO"
  format: "json"
  file: "/var/log/rag-system/app.log"
